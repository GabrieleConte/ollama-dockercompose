
services:
  frontend:
    build:
      context: ./fe_chatbot
    container_name: frontend
    ports:
      - "80:80"
    depends_on:
      - app
    networks:
      - webapp
  app:
    build: ./be_chatbot/ 
    volumes:
      - ./be_chatbot/src:/code/src
      - ./be_chatbot/requirements.txt:/code/requirements.txt
    command: uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload
    env_file:
      - ./be_chatbot/.env.config
      - ./be_chatbot/.env.secrets
    restart: always
    depends_on:
          ollama:
            condition: service_healthy  # Wait until ollama is healthy
    networks:
      - ollama-docker
      - webapp
    expose:
      - "8000"
 
  ollama:
    image: ollama/ollama:latest
    env_file:
      - ./be_chatbot/.env.config
    volumes:
      - ./scripts:/scripts
      - ./be_chatbot/ollama/ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: always
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    entrypoint: ["/usr/bin/bash", "/scripts/download_models.sh"] # Download the model
    healthcheck:
      test : ["CMD",/usr/bin/bash,"./scripts/healthcheck.sh"]  # Example health check command
      interval: 60s  # Check every 30 seconds
      timeout: 5s    # Timeout for each check
      retries: 10   # will timeout after 3 retries
      start_period: 300s  # Initial delay before health checks start, will timeout after 15 minutes, increase if the model takes longer to load
    networks:
      - ollama-docker
    expose:
      - "11434"

networks:
  webapp:
    external: false
  ollama-docker:
    external: false
  
